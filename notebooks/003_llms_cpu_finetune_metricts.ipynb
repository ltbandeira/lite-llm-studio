{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e12089",
   "metadata": {},
   "source": [
    "# Notebook Description\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9221e59",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dea20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, threading, psutil, gc, dotenv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from huggingface_hub import HfFolder\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222cc516",
   "metadata": {},
   "source": [
    "## Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o ambiente para CPU\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"18\"  # Número de threads para OpenMP\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"18\"  # Número de threads para MKL\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Garante que os processos não usem GPU\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "torch.set_num_threads(int(os.environ[\"OMP_NUM_THREADS\"]))\n",
    "\n",
    "print(\"CUDA disponível?\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dbf8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suprime verbose do Transformers, PyTorch e Hugging Face\n",
    "\n",
    "# Desabilita as barras de progresso do Hugging Face\n",
    "# os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "# logging.set_verbosity_error()\n",
    "# logging.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4150f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o token do huggingface\n",
    "\n",
    "# Carrega as variáveis de ambiente do arquivo .env\n",
    "dotenv.load_dotenv(\"../../env/.env\")\n",
    "access_token = os.getenv(\"HF_TOKEN\")\n",
    "if not access_token:\n",
    "    raise RuntimeError(\"HF_TOKEN não definido!\")\n",
    "\n",
    "# Salva o token no cache do huggingface\n",
    "HfFolder.save_token(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb28f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o fine-tuning do modelo\n",
    "\n",
    "# Nome do modelo base\n",
    "MODEL_NAMES = [\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "]\n",
    "\n",
    "# Parâmetros do fine-tuning\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 1\n",
    "BLOCK_SIZE = 128\n",
    "SAMPLE_INTERVAL = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee93575",
   "metadata": {},
   "source": [
    "## Resource Monitoring Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ed4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceMonitor:\n",
    "    def __init__(self, interval: float = 0.1):\n",
    "        self.interval = interval\n",
    "        self._stop_event = threading.Event()\n",
    "        self._thread = threading.Thread(target=self._run, daemon=True)\n",
    "        self.samples: List[Dict[str, Any]] = []\n",
    "        self._baseline_mem_mb = 0.0\n",
    "\n",
    "    def _run(self):\n",
    "        proc = psutil.Process()\n",
    "        while not self._stop_event.is_set():\n",
    "            # Por processo\n",
    "            raw_mem = proc.memory_info().rss / 1024**2\n",
    "            mem_proc = max(raw_mem - self._baseline_mem_mb, 0.0)\n",
    "            cpu_proc = proc.cpu_percent(None)\n",
    "            timestamp = time.perf_counter()\n",
    "            \n",
    "            self.samples.append(\n",
    "                {\n",
    "                    \"time\": timestamp,\n",
    "                    \"cpu_proc_pct\": cpu_proc,\n",
    "                    \"mem_proc_mb\": mem_proc,\n",
    "                }\n",
    "            )\n",
    "            time.sleep(self.interval)\n",
    "\n",
    "    def start(self):\n",
    "        self.samples = []\n",
    "        self._stop_event.clear()\n",
    "        proc = psutil.Process()\n",
    "        proc.cpu_percent(None)\n",
    "        self._baseline_mem_mb = proc.memory_info().rss / 1024**2\n",
    "        self._thread = threading.Thread(target=self._run, daemon=True)\n",
    "        self._thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self._stop_event.set()\n",
    "        if self._thread is not None:\n",
    "            self._thread.join()\n",
    "        return self.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465486aa",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os dados do dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"yale-lily/aeslc\", split=\"train\")\n",
    "raw_datasets = raw_datasets.rename_column(\"email_body\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c76f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções de pré-processamento dos dados\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=BLOCK_SIZE)\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated = []\n",
    "    for ids in examples[\"input_ids\"]:\n",
    "        concatenated.extend(ids)\n",
    "    total_length = (len(concatenated) // BLOCK_SIZE) * BLOCK_SIZE\n",
    "    result = {\n",
    "        \"input_ids\": [\n",
    "            concatenated[i : i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)\n",
    "        ]\n",
    "    }\n",
    "    result[\"attention_mask\"] = [\n",
    "        [1] * BLOCK_SIZE for _ in range(len(result[\"input_ids\"]))\n",
    "    ]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec550de4",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c76101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(\n",
    "    model_name: str,\n",
    "    dataset,\n",
    "    epochs: int = EPOCHS,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    block_size: int = BLOCK_SIZE,\n",
    "    sample_interval: float = SAMPLE_INTERVAL,\n",
    ") -> dict:\n",
    "    # Carrega tokenizer e modelo\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, token=access_token, padding_side=\"right\", truncation_side=\"right\"\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, use_cache=False, token=access_token, torch_dtype=torch.float16, device_map=\"cpu\", low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    # model = torch.quantization.quantize_dynamic(\n",
    "    #     model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "    # )\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Tokeniza e agrupa datasets\n",
    "    tokenized = dataset.map(lambda examples: tokenize_function(examples, tokenizer), batched=True, remove_columns=[\"text\"])\n",
    "    \n",
    "    if len(tokenized) == 0:\n",
    "        raise ValueError(\"Dataset vazio após tokenização!\")\n",
    "    \n",
    "    lm_datasets = tokenized.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        batch_size=len(tokenized),\n",
    "        remove_columns=tokenized.column_names,\n",
    "    )\n",
    "    \n",
    "    print(f\"Total de amostras: {len(lm_datasets)}\")\n",
    "    print(f\"Amostra de dados: {lm_datasets[0]}\")\n",
    "    print(f\"Exemplo de tokenização: {tokenizer.decode(lm_datasets[0]['input_ids'])}\")\n",
    "\n",
    "    # DataCollator\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # TrainingArguments\n",
    "    output_dir = f\"output/{model_name.replace('/', '_')}_finetuned_enron\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=1,\n",
    "        logging_steps=500,\n",
    "        logging_dir=f\"logs/{model_name.replace('/', '_')}_enron\",\n",
    "        no_cuda=True,\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=lm_datasets,\n",
    "    )\n",
    "\n",
    "    # Inicia monitor e tempo\n",
    "    monitor = ResourceMonitor(interval=sample_interval)\n",
    "    monitor.start()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Execução do treinamento\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    # Para monitoramento\n",
    "    samples = monitor.stop()\n",
    "    total_time = time.perf_counter() - t0\n",
    "    cpu_peak = max(s[\"cpu_proc_pct\"] for s in samples) if samples else 0.0\n",
    "    mem_peak = max(s[\"mem_proc_mb\"] for s in samples) if samples else 0.0\n",
    "\n",
    "    # Limpa memória\n",
    "    del model, tokenizer, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"train_time_s\": total_time,\n",
    "        \"peak_cpu_proc_pct\": cpu_peak,\n",
    "        \"peak_ram_proc_mb\": mem_peak,\n",
    "        \"train_metrics\": train_result.metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1576bf",
   "metadata": {},
   "source": [
    "## Generate Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f83abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# print(torch.cuda.is_available())\n",
    "# print(torch.cuda.current_device())    # índice da GPU em uso\n",
    "# print(torch.cuda.memory_allocated())  # memória alocada em bytes\n",
    "print(raw_datasets)\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    print(f\"\\n---- Fine-tuning Enron: {model_name} ----\")\n",
    "    \n",
    "    try:\n",
    "        metrics = fine_tune_model(\n",
    "            model_name=model_name,\n",
    "            dataset=raw_datasets,\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            block_size=BLOCK_SIZE,\n",
    "            sample_interval=SAMPLE_INTERVAL,\n",
    "        )\n",
    "        results.append(metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao treinar {model_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4020807",
   "metadata": {},
   "source": [
    "## Analyse Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f9233",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "os.makedirs(\"../../data/raw/ft_models\", exist_ok=True)\n",
    "df_results.to_csv(\"../../data/raw/ft_models/fine_tuning_enron_metrics.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
